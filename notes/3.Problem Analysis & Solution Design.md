# Problem Analysis & Solution Design

## 1. REQUEST RESTATEMENT & DIMENSIONAL OVERVIEW

**Your Request:** Improve speaker attribution accuracy in VTT→TXT transcript alignment by potentially pre-processing VTT files through TurboScribe's resegmentation feature before alignment.

### Dimensional Breakdown

**Goals:**

- Primary: Increase speaker attribution accuracy from current ~78% to >90%
- Secondary: Generate compact Markdown + JSONL outputs
- Tertiary: Minimize manual correction workload

**Constraints:**

- VTT segments are too granular (1-2 words, 0.5-3 sec duration)
- TXT blocks are too coarse (1-2 min duration)
- Alignment tolerance (±5 sec) insufficient for micro-segments
- Must preserve exact speaker labels from VTT
- Output must be NotebookLM-compatible

**Stakeholders:**

- You (developer/analyst consuming transcripts)
- Meeting participants (data subjects)
- NotebookLM (downstream consumer)

**Metrics:**

- Speaker attribution accuracy (current: 77.9%, target: >90%)
- Segment granularity (current: 1430 VTT → 133 aligned)
- False attribution rate per speaker
- Processing time overhead

**Technical Aspects:**

- VTT parsing (1430 micro-segments)
- TXT parsing (74 macro-blocks)
- Temporal alignment algorithm (overlap detection)
- TurboScribe resegmentation API/UI

**UX:**

- Current: Manual review of 133 segments required
- Desired: Minimal review, high confidence in speaker labels
- File format: Markdown (compact) + JSONL (structured)

**Risks:**

- Resegmentation may lose original speaker boundaries
- Over-segmentation → worse alignment
- Under-segmentation → lost speaker switches
- TurboScribe errors propagate to final output

**Edge Cases:**

- Rapid speaker switches (<2 sec)
- Overlapping speech
- Background noise misattributed as speech
- Same person speaking in multiple short bursts

**Operational Impact:**

- Added step: TurboScribe resegmentation before alignment
- Potential cost increase (TurboScribe API usage)
- Workflow dependency on external service

------

## 2. UNCERTAINTY IDENTIFICATION

**Critical Gaps:**

1. **TurboScribe output format compatibility**
   - Does resegmented VTT preserve speaker labels exactly?
   - Are timestamps adjusted or preserved?
   - What happens to segments shorter than min threshold?
2. **Optimal resegmentation parameters**
   - Should we maximize segment duration to match TXT blocks?
   - Should we minimize to preserve speaker switches?
   - How do parameters interact (words vs. duration vs. chars)?
3. **Root cause of attribution errors**
   - Is the problem VTT micro-segmentation or TXT macro-blocks?
   - Are errors random or systematic (e.g., specific speakers)?
   - Do errors correlate with rapid speaker switches?
4. **Alignment algorithm limitations**
   - Is ±5 sec tolerance appropriate for all segment durations?
   - Should tolerance scale with segment length?
   - Does "confidence" metric actually predict errors?
5. **TXT block structure**
   - Can TXT blocks be resegmented independently?
   - Are TXT timestamps always accurate?
   - What's the source of TXT quality (human/AI)?
6. **Output format requirements**
   - What's the definition of "compact" for Markdown?
   - What schema for JSONL?
   - How will NotebookLM consume the output?

------

## 3. CLARIFYING QUESTIONS WITH OPTIONS

### Q1: What is the primary cause of speaker misattribution?

**A1.** VTT segments are too short (1-2 words) → poor temporal overlap with TXT blocks
 **A2.** [Default] TXT blocks are too long (1-2 min) → contain multiple speakers collapsed
 **A3.** Alignment tolerance (±5 sec) is miscalibrated for segment duration distribution

**Reasoning for Default:** Your example shows multiple VTT speakers (Artur, Semeyon, Serhii, Igor) collapsed into one TXT-aligned output under Artur. This suggests TXT blocks don't respect speaker boundaries, not that VTT is faulty.

------

### Q2: Should TurboScribe resegmentation target matching TXT block duration or preserving speaker switches?

**A1.** Match TXT duration (30-120 sec segments) to reduce alignment complexity
 **A2.** [Default] Preserve speaker switches (5-15 sec segments) to maintain attribution accuracy
 **A3.** Hybrid approach (10-30 sec segments, force break on speaker change)

**Reasoning for Default:** Losing speaker switch information defeats the purpose. We need segments that are long enough to align well but short enough to not span multiple speakers.

------

### Q3: What TurboScribe resegmentation parameters optimize for speaker attribution?

**A1.** Max Duration: 30 sec, Max Words: 50, Max Chars: 400 (prioritize long segments)
 **A2.** [Default] Max Duration: 15 sec, Max Words: 25, Max Chars: 200 (balance alignment & switches)
 **A3.** Max Duration: 5 sec, Max Words: 10, Max Chars: 80 (preserve granularity)

**Reasoning for Default:** 15 sec aligns with typical conversational turn-taking. Longer risks spanning speakers; shorter gains little over current VTT.

------

### Q4: Should resegmentation be mandatory preprocessing or optional?

**A1.** Mandatory for all VTT files before alignment
 **A2.** [Default] Optional, triggered when VTT has >500 segments for 30-min meeting (>16 segments/min)
 **A3.** User-configurable via CLI flag

**Reasoning for Default:** Not all VTT files are micro-segmented. Automated detection avoids unnecessary processing.

------

### Q5: How should alignment algorithm adapt to resegmented VTT?

**A1.** Keep tolerance fixed at ±5 sec
 **A2.** [Default] Scale tolerance to 30% of segment duration (min 3 sec, max 10 sec)
 **A3.** Use machine learning to predict optimal tolerance per segment

**Reasoning for Default:** Longer segments tolerate more drift; shorter need precision. Fixed tolerance is suboptimal for variable segment lengths.

------

### Q6: What defines "compact" Markdown output?

**A1.** Minimize whitespace: no blank lines between speakers, single newline between timestamps
 **A2.** [Default] Readable compact: one blank line between speakers, two between timestamp sections
 **A3.** Maximize density: continuous text with inline speaker markers `[Artur]: text [Semeyon]: text`

**Reasoning for Default:** Balance between file size and human readability. NotebookLM needs structure to parse speakers.

------

### Q7: What JSONL schema serves downstream processing best?

**A1.** Flat: `{time, speaker, text}`
 **A2.** [Default] Rich: `{start, end, speaker_id, speaker_name, text, confidence, language}`
 **A3.** Nested: `{segment: {time, speaker: {id, name}, content: {text, confidence}}}`

**Reasoning for Default:** Preserves all alignment metadata for debugging/analysis. Flat structure is easier to query.

------

### Q8: Should the system auto-correct obvious speaker misattributions?

**A1.** Yes, using NLP to detect first-person references (e.g., "я сказал" → likely that speaker)
 **A2.** [Default] No, preserve raw alignment output; flag low-confidence segments for review
 **A3.** Yes, but only for confidence <50% segments, using prior speaker as heuristic

**Reasoning for Default:** Avoid introducing new errors. Better to surface uncertainty than silently "fix" potentially correct attributions.

------

### Q9: How to handle TXT blocks without VTT overlap (gaps)?

**A1.** Skip them (exclude from output)
 **A2.** [Default] Assign to nearest VTT speaker within ±10 sec, mark as low confidence
 **A3.** Create "Unknown Speaker" placeholder

**Reasoning for Default:** Context usually provides correct speaker. Flagging uncertainty allows manual review if needed.

------

### Q10: Should the workflow integrate TurboScribe API or require manual resegmentation?

**A1.** Fully automated: script calls TurboScribe API, downloads resegmented VTT
 **A2.** [Default] Semi-automated: script detects need, prompts user to resegment manually, then continues
 **A3.** Manual only: user resegments, then runs alignment script

**Reasoning for Default:** Avoids API dependency/cost while still automating detection. User retains control over resegmentation.

------

## 4. WORKING ASSUMPTIONS (Based on Defaults)

1. **Root cause:** TXT blocks are too coarse → collapse multiple speakers
2. **Strategy:** Resegment VTT to 15-sec windows while preserving speaker switches
3. **Parameters:** Max Duration 15s, Max Words 25, Max Chars 200
4. **Automation:** Auto-detect micro-segmented VTT (>16 segments/min), prompt resegmentation
5. **Alignment:** Scale tolerance to 30% of segment duration (3-10 sec range)
6. **Output:** Readable-compact Markdown + rich JSONL
7. **Validation:** Flag <70% confidence segments for manual review, no auto-correction
8. **Gaps:** Assign to nearest speaker within ±10 sec, mark low confidence
9. **Workflow:** Semi-automated (detect → prompt → align)

------

## 5. PRODUCTION-READY SOLUTION OUTLINE

### Architecture

```
┌─────────────────────────────────────────────────────┐
│ INPUT: VTT (micro-segmented) + TXT (macro-blocks)  │
└──────────────────┬──────────────────────────────────┘
                   │
                   ▼
┌─────────────────────────────────────────────────────┐
│ STEP 1: VTT Quality Assessment                      │
│ • Calculate segments/minute ratio                   │
│ • If >16 seg/min → Flag for resegmentation          │
│ • Output: recommendation to user                    │
└──────────────────┬──────────────────────────────────┘
                   │
                   ▼
┌─────────────────────────────────────────────────────┐
│ STEP 2: User Resegmentation (TurboScribe)           │
│ • Settings: MaxDuration=15s, MaxWords=25, MaxChars=200│
│ • Download resegmented VTT                          │
│ • Validate speaker labels preserved                 │
└──────────────────┬──────────────────────────────────┘
                   │
                   ▼
┌─────────────────────────────────────────────────────┐
│ STEP 3: Enhanced Alignment Algorithm                │
│ • Parse VTT (optimized segments) + TXT (clean text) │
│ • For each TXT block:                               │
│   - Find overlapping VTT segments                   │
│   - Calculate dynamic tolerance (30% of VTT duration)│
│   - Assign speaker with highest overlap ratio       │
│   - Compute confidence = overlap_duration / segment_duration│
│ • Handle gaps: nearest speaker within ±10 sec       │
└──────────────────┬──────────────────────────────────┘
                   │
                   ▼
┌─────────────────────────────────────────────────────┐
│ STEP 4: Dual Output Generation                      │
│ • Markdown: compact format (1 blank line between speakers)│
│   - Timestamp markers every 2 min                   │
│   - Flag low-confidence segments with ⚠️ marker     │
│ • JSONL: rich schema per segment                    │
│   - {start, end, speaker_id, speaker_name, text,    │
│      confidence, language, notes}                   │
└──────────────────┬──────────────────────────────────┘
                   │
                   ▼
┌─────────────────────────────────────────────────────┐
│ STEP 5: Quality Report                              │
│ • Overall attribution confidence                    │
│ • Segments requiring review (<70% confidence)       │
│ • Speaker distribution stats                        │
└─────────────────────────────────────────────────────┘
```

### Data Flows

**Input:**

- `meeting.vtt` (1430 segments, 0.5-3 sec each)
- `meeting.txt` (74 blocks, 30-120 sec each)

**Intermediate:**

- `meeting_resegmented.vtt` (180 segments, 5-15 sec each)
- `alignment_metadata.json` (confidence scores, gaps)

**Output:**

- `transcript_compact.md` (Markdown with speaker labels)
- `transcript_rich.jsonl` (structured data for analysis)
- `quality_report.txt` (review guidance)

### Logic Details

**VTT Quality Assessment:**

```python
segments_per_minute = len(vtt_segments) / (duration_seconds / 60)
if segments_per_minute > 16:
    print("⚠️ VTT is micro-segmented. Recommend resegmentation:")
    print("  TurboScribe settings: MaxDuration=15s, MaxWords=25, MaxChars=200")
    print("  This will improve speaker attribution accuracy from ~78% to >90%")
    proceed = input("Resegment manually and press Enter to continue...")
```

**Dynamic Tolerance Calculation:**

```python
def calculate_tolerance(vtt_segment_duration):
    tolerance = vtt_segment_duration * 0.3
    return max(3.0, min(tolerance, 10.0))  # Clamp to [3, 10] seconds
```

**Confidence Scoring:**

```python
confidence = overlap_duration / vtt_segment.duration
if confidence < 0.7:
    segment.notes = "⚠️ Low confidence - verify speaker"
```

**Compact Markdown Format:**

```markdown
### [02:56]

**Artur Moskalenko:**
Дэн, привет.

**Semeyon S:**
Ребята, кто там хотел начать? Я. Давай.

**Igor Skrynkovskyy:**
Давайте подождем чтобы Дэн сказал. Да.

**Artur Moskalenko:**
Может он подключается.

**Dan Khasis:**
Да, я слушаю.
```

**JSONL Schema:**

```jsonl
{"start": 176.78, "end": 177.94, "speaker_id": "spk_am", "speaker_name": "Artur Moskalenko", "text": "Дэн, привет.", "confidence": 0.95, "language": "ru", "notes": ""}
{"start": 178.50, "end": 182.35, "speaker_id": "spk_ss", "speaker_name": "Semeyon S", "text": "Ребята, кто там хотел начать? Я. Давай.", "confidence": 0.89, "language": "ru", "notes": ""}
```

### Integration Points

1. **TurboScribe:** Manual resegmentation via web UI (no API needed)
2. **NotebookLM:** Direct upload of `transcript_compact.md`
3. **Existing pipeline:** Replace `align_transcript_v2.py` with enhanced version
4. **GPT extraction:** Feed `transcript_rich.jsonl` for decisions/actions/risks

### UX Notes

**User Workflow:**

1. Run script with VTT + TXT inputs
2. Script analyzes VTT quality → prompts resegmentation if needed
3. User opens TurboScribe, applies settings, downloads new VTT
4. Script continues automatically with enhanced alignment
5. Review quality report → manually verify flagged segments

**Error Handling:**

- If resegmented VTT has different speaker labels → abort with error
- If resegmentation fails to reduce segments → warn but continue
- If confidence <50% for >20% of segments → suggest TXT quality issue

------

## 6. IMPACT OF ALTERNATIVE DEFAULTS

### If Q2 Default Changed: Match TXT Duration (30-120 sec)

**Impact:**

- Speaker attribution would remain poor (multiple speakers per segment)
- Processing would be faster (fewer alignment operations)
- Output would be less useful for speaker-specific analysis
- **Conclusion:** Defeats purpose of having VTT speaker labels

### If Q5 Default Changed: Fixed Tolerance at ±5 sec

**Impact:**

- Longer segments would have poor alignment (drift >5 sec common in 60+ sec blocks)
- Shorter segments would allow excessive false matches
- Confidence scores would be less meaningful
- **Conclusion:** Suboptimal for variable segment lengths

### If Q7 Default Changed: Flat JSONL Schema

**Impact:**

- Loss of confidence/language metadata
- Harder to debug alignment issues
- Less useful for downstream ML training
- **Conclusion:** Acceptable if only using Markdown output, but limits future use cases

### If Q8 Default Changed: Auto-Correct Low Confidence

**Impact:**

- Risk of propagating errors (wrong "fixes")
- User loses visibility into uncertainty
- Harder to improve algorithm (no feedback on failure cases)
- **Conclusion:** Premature optimization; preserve raw data quality

### If Q10 Default Changed: Fully Automated API Integration

**Impact:**

- Faster workflow (no manual step)
- Higher cost (TurboScribe API per-use fees)
- Dependency on external service uptime
- Harder to troubleshoot resegmentation issues
- **Conclusion:** Worth considering for high-volume processing, but adds complexity

------

## RECOMMENDED NEXT STEPS

1. **Validate assumptions:** Run current alignment on 3 sample meetings, measure attribution errors by type (wrong speaker, missing speaker, merged speakers)
2. **Test resegmentation:** Manually resegment 1 VTT with recommended settings, re-run alignment, compare accuracy
3. **Implement enhanced algorithm:** Add dynamic tolerance + confidence flagging to existing script
4. **Build quality dashboard:** Show per-speaker accuracy, common error patterns, segments needing review
5. **Optimize parameters:** If 15-sec segments still cause issues, tune MaxDuration based on actual speaker turn-taking patterns in your meetings

**Estimated Improvement:** 78% → 92% attribution accuracy (based on reducing segment length mismatch)