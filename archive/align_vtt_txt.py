#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ñ–∏—á–∞–º–∏
"""

import re
from typing import List, Tuple, Dict
from dataclasses import dataclass
from collections import Counter
from datetime import datetime


@dataclass
class VTTSegment:
    start: float
    end: float
    speaker: str
    text: str


@dataclass
class TXTBlock:
    start: float
    end: float
    text: str


@dataclass
class AlignedSegment:
    start: float
    end: float
    speaker: str
    text: str
    confidence: float = 1.0


def parse_timestamp(ts: str) -> float:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ —Å–µ–∫—É–Ω–¥—ã"""
    parts = ts.split(':')
    if len(parts) == 3:
        h, m, s = parts
        return int(h) * 3600 + int(m) * 60 + float(s)
    elif len(parts) == 2:
        m, s = parts
        return int(m) * 60 + float(s)
    return float(ts)


def parse_vtt(filepath: str) -> List[VTTSegment]:
    """–ü–∞—Ä—Å–∏–Ω–≥ VTT —Ñ–∞–π–ª–∞"""
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    
    segments = []
    blocks = re.split(r'\n\n+', content)
    
    for block in blocks:
        if not block.strip() or block.strip() == 'WEBVTT':
            continue
        
        lines = block.strip().split('\n')
        timestamp_line = None
        text_lines = []
        
        for line in lines:
            if '-->' in line:
                timestamp_line = line
            elif line.strip() and not line.strip().isdigit():
                text_lines.append(line.strip())
        
        if not timestamp_line or not text_lines:
            continue
        
        match = re.search(r'(\d{2}:\d{2}:\d{2}\.\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2}\.\d{3})', timestamp_line)
        if not match:
            continue
        
        start = parse_timestamp(match.group(1))
        end = parse_timestamp(match.group(2))
        
        first_line = text_lines[0]
        speaker_match = re.match(r'^([^:]+):\s*(.*)$', first_line)
        
        if speaker_match:
            speaker = speaker_match.group(1).strip()
            text = speaker_match.group(2)
            if len(text_lines) > 1:
                text = text + ' ' + ' '.join(text_lines[1:])
        else:
            speaker = "Unknown"
            text = ' '.join(text_lines)
        
        segments.append(VTTSegment(start=start, end=end, speaker=speaker, text=text))
    
    return segments


def parse_txt(filepath: str) -> List[TXTBlock]:
    """–ü–∞—Ä—Å–∏–Ω–≥ TXT —Ñ–∞–π–ª–∞"""
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    
    blocks = []
    pattern = r'\((\d+:\d+)\s*-\s*(\d+:\d+)\)\s*\n(.*?)(?=\n\(\d+:\d+\s*-|\Z)'
    matches = re.finditer(pattern, content, re.DOTALL)
    
    for match in matches:
        start = parse_timestamp(match.group(1))
        end = parse_timestamp(match.group(2))
        text = match.group(3).strip()
        blocks.append(TXTBlock(start=start, end=end, text=text))
    
    return blocks


def split_text_smartly(text: str) -> List[str]:
    """–£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ñ—Ä–∞–∑—ã"""
    # –†–∞–∑–±–∏—Ç—å –ø–æ —Ç–æ—á–∫–∞–º, –≤–æ–ø—Ä–æ—Å–∞–º, –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è–º
    sentences = re.split(r'(?<=[.!?])\s+', text)
    
    # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã
    result = []
    buffer = ""
    
    for sent in sentences:
        sent = sent.strip()
        if not sent:
            continue
        
        if len(buffer) > 0:
            buffer += " " + sent
        else:
            buffer = sent
        
        # –ï—Å–ª–∏ –±—É—Ñ–µ—Ä –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–π –∏–ª–∏ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        if len(buffer) > 100 or sent == sentences[-1]:
            result.append(buffer)
            buffer = ""
    
    if buffer:
        result.append(buffer)
    
    return result


def align_segments(vtt_segments: List[VTTSegment], txt_blocks: List[TXTBlock]) -> List[AlignedSegment]:
    """–í—ã—Ä–æ–≤–Ω—è—Ç—å VTT —Å–ø–∏–∫–µ—Ä–æ–≤ —Å TXT —Ç–µ–∫—Å—Ç–æ–º"""
    aligned = []
    
    for txt_block in txt_blocks:
        # –ù–∞–π—Ç–∏ VTT —Å–µ–≥–º–µ–Ω—Ç—ã –≤ —ç—Ç–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
        tolerance = 5.0  # —Å–µ–∫—É–Ω–¥—ã
        overlapping_vtt = []
        
        for vtt in vtt_segments:
            if not (vtt.end < txt_block.start - tolerance or vtt.start > txt_block.end + tolerance):
                overlap_start = max(vtt.start, txt_block.start)
                overlap_end = min(vtt.end, txt_block.end)
                overlap_duration = max(0, overlap_end - overlap_start)
                
                if overlap_duration > 0:
                    overlapping_vtt.append((vtt, overlap_duration))
        
        if not overlapping_vtt:
            continue
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        overlapping_vtt.sort(key=lambda x: x[0].start)
        
        # –†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —É–º–Ω—ã–µ —Ñ—Ä–∞–∑—ã
        phrases = split_text_smartly(txt_block.text)
        
        if not phrases:
            continue
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ñ—Ä–∞–∑—ã –ø–æ VTT —Å–µ–≥–º–µ–Ω—Ç–∞–º
        total_vtt_duration = sum(overlap for _, overlap in overlapping_vtt)
        chars_per_second = len(txt_block.text) / total_vtt_duration if total_vtt_duration > 0 else 0
        
        phrase_idx = 0
        
        for vtt, overlap in overlapping_vtt:
            if phrase_idx >= len(phrases):
                break
            
            # –°–∫–æ–ª—å–∫–æ —Å–∏–º–≤–æ–ª–æ–≤ –º–æ–∂–µ–º —É–º–µ—Å—Ç–∏—Ç—å
            vtt_duration = vtt.end - vtt.start
            estimated_chars = int(vtt_duration * chars_per_second * 1.3)  # +30% –±—É—Ñ–µ—Ä
            
            segment_text = ""
            phrases_used = 0
            
            while phrase_idx < len(phrases) and len(segment_text + phrases[phrase_idx]) <= estimated_chars:
                if segment_text:
                    segment_text += " "
                segment_text += phrases[phrase_idx]
                phrase_idx += 1
                phrases_used += 1
            
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —É–º–µ—Å—Ç–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π —Ñ—Ä–∞–∑—ã, –±–µ—Ä–µ–º —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É
            if not segment_text and phrase_idx < len(phrases):
                segment_text = phrases[phrase_idx]
                phrase_idx += 1
                phrases_used = 1
            
            if segment_text:
                # –†–∞—Å—Å—á–∏—Ç–∞—Ç—å confidence –Ω–∞ –æ—Å–Ω–æ–≤–µ overlap
                confidence = min(1.0, overlap / vtt_duration) if vtt_duration > 0 else 0.5
                
                aligned.append(AlignedSegment(
                    start=vtt.start,
                    end=vtt.end,
                    speaker=vtt.speaker,
                    text=segment_text,
                    confidence=confidence
                ))
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    aligned.sort(key=lambda x: x.start)
    
    return aligned


def format_time(seconds: float) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ MM:SS"""
    m = int(seconds // 60)
    s = int(seconds % 60)
    return f"{m:02d}:{s:02d}"


def format_time_verbose(seconds: float) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ HH:MM:SS"""
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    return f"{h:02d}:{m:02d}:{s:02d}"


def generate_markdown(segments: List[AlignedSegment]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Markdown —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞"""
    lines = []
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    lines.append("# –í—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –≤—Å—Ç—Ä–µ—á–∏")
    lines.append("")
    lines.append(f"**–î–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:** {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    
    if segments:
        total_duration = segments[-1].end - segments[0].start
        lines.append(f"**–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** {format_time_verbose(total_duration)}")
        
        speakers = set(seg.speaker for seg in segments)
        lines.append(f"**–£—á–∞—Å—Ç–Ω–∏–∫–∏:** {len(speakers)}")
    
    lines.append("")
    lines.append("---")
    lines.append("")
    
    # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
    last_speaker = None
    last_timestamp = 0
    
    for seg in segments:
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∫–∞–∂–¥—ã–µ 2 –º–∏–Ω—É—Ç—ã
        if seg.start - last_timestamp >= 120:
            lines.append("")
            lines.append(f"### [{format_time(seg.start)}]")
            lines.append("")
            last_timestamp = seg.start
        
        # –°–º–µ–Ω–∞ —Å–ø–∏–∫–µ—Ä–∞
        if seg.speaker != last_speaker:
            lines.append("")
            lines.append(f"**{seg.speaker}:**  ")
            last_speaker = seg.speaker
        
        lines.append(seg.text)
        lines.append("")
    
    return '\n'.join(lines)


def generate_statistics(segments: List[AlignedSegment]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
    lines = []
    
    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append("## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
    lines.append("")
    
    # –°–ø–∏–∫–µ—Ä—ã
    speaker_stats = Counter(seg.speaker for seg in segments)
    speaker_times = {}
    
    for seg in segments:
        duration = seg.end - seg.start
        speaker_times[seg.speaker] = speaker_times.get(seg.speaker, 0) + duration
    
    lines.append("### –£—á–∞—Å—Ç–Ω–∏–∫–∏")
    lines.append("")
    
    for speaker, count in speaker_stats.most_common():
        time = speaker_times[speaker]
        percentage = (time / sum(speaker_times.values())) * 100
        lines.append(f"- **{speaker}**: {count} —Å–µ–≥–º–µ–Ω—Ç–æ–≤, {format_time_verbose(time)} ({percentage:.1f}%)")
    
    lines.append("")
    
    # –û–±—â–∞—è –∏–Ω—Ñ–æ
    lines.append("### –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
    lines.append("")
    lines.append(f"- –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segments)}")
    lines.append(f"- –í—Å–µ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: {len(speaker_stats)}")
    
    if segments:
        total_duration = segments[-1].end - segments[0].start
        lines.append(f"- –û–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {format_time_verbose(total_duration)}")
        
        avg_confidence = sum(seg.confidence for seg in segments) / len(segments)
        lines.append(f"- –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è: {avg_confidence:.1%}")
    
    lines.append("")
    
    return '\n'.join(lines)


def main():
    print("=" * 80)
    print("–£–õ–£–ß–®–ï–ù–ù–û–ï –í–´–†–ê–í–ù–ò–í–ê–ù–ò–ï VTT + TXT")
    print("=" * 80)
    print()
    
    # –ü—É—Ç–∏
    vtt_path = "/home/vadim/Projects/route4me.com/turboscribe/zoom.vtt"
    txt_path = "/home/vadim/Projects/route4me.com/turboscribe/GMT20251106-142611 Recording/GMT20251106-142611 Recording.txt"
    output_path = "/home/vadim/Projects/route4me.com/turboscribe/resegmented_transcript.md"
    
    # –ü–∞—Ä—Å–∏–Ω–≥
    print("üìñ –ß—Ç–µ–Ω–∏–µ VTT —Ñ–∞–π–ª–∞...")
    vtt_segments = parse_vtt(vtt_path)
    print(f"   ‚úì {len(vtt_segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å —Å–ø–∏–∫–µ—Ä–∞–º–∏")
    
    print("üìñ –ß—Ç–µ–Ω–∏–µ TXT —Ñ–∞–π–ª–∞...")
    txt_blocks = parse_txt(txt_path)
    print(f"   ‚úì {len(txt_blocks)} –±–ª–æ–∫–æ–≤ —Å —á–∏—Å—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–º")
    
    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ
    print("\nüîÑ –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –º–µ—Ç–∫–∞–º...")
    aligned = align_segments(vtt_segments, txt_blocks)
    print(f"   ‚úì {len(aligned)} –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    print("\nüìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Markdown...")
    markdown = generate_markdown(aligned)
    statistics = generate_statistics(aligned)
    
    full_content = markdown + "\n" + statistics
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(full_content)
    
    print(f"   ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {output_path}")
    
    # –ö—Ä–∞—Ç–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "=" * 80)
    print("–ö–†–ê–¢–ö–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 80)
    
    speaker_counts = Counter(seg.speaker for seg in aligned)
    print(f"\nüë• –£—á–∞—Å—Ç–Ω–∏–∫–∏ ({len(speaker_counts)}):")
    for speaker, count in speaker_counts.most_common():
        print(f"   {speaker}: {count} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
    
    if aligned:
        total_duration = aligned[-1].end - aligned[0].start
        print(f"\n‚è± –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {format_time_verbose(total_duration)}")
        avg_conf = sum(s.confidence for s in aligned) / len(aligned)
        print(f"‚úÖ –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_conf:.1%}")
    
    print(f"\n‚úì –ì–æ—Ç–æ–≤–æ! –§–∞–π–ª: {output_path}")
    print()


if __name__ == "__main__":
    main()
